# RH-inference-server

This repo describe the steps needed to deploy a pod with inference server on OpenShift.
Based on the instuctions [here](https://docs.redhat.com/en/documentation/red_hat_ai_inference_server/3.0/html-single/getting_started/index) I have conducted two experiments of setting the inference server:
1. As a deployment.
2. As a pod (server).
3. As a service.

More info can be found under the relevant directories.


