apiVersion: v1
kind: Pod
metadata:
  name: test-vllm-image
  namespace: hacohen-inference-server
spec:
  tolerations:
      - key: "g5-gpu"
        operator: "Exists"
        effect: "NoSchedule"
  securityContext:
    seccompProfile:
      type: RuntimeDefault
  containers:
  - name: test
    image: registry.redhat.io/rhaiis/vllm-cuda-rhel9:3.0.0
    resources:
      requests:
        memory: "4Gi"
        cpu: "4"
        nvidia.com/gpu: "1"
      limits:
        memory: "8Gi"
        cpu: "8"
        nvidia.com/gpu: "1"
    env:
      - name: VLLM_LOGGING_LEVEL
        value: DEBUG
    securityContext:
      allowPrivilegeEscalation: false
      runAsNonRoot: true
      capabilities:
        drop:
          - ALL
  restartPolicy: Never
