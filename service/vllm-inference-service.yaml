# vllm-inference-service.yaml (Corrected - storageUri removed)
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: llama-3-2-1b-instruct
  namespace: hacohen-inference-server # Your OpenShift project/namespace
spec:
  predictor:
    model:
      modelFormat:
        name: vllm
        version: "1"
      # Specify the custom ServingRuntime
      runtime: vllm-cuda-runtime # This must match the name of the ServingRuntime defined above
      # REMOVED THE 'storageUri' AND 'storage' BLOCKS ENTIRELY
      resources:
        limits:
          cpu: "8"
          memory: "8Gi"
          nvidia.com/gpu: "1"
        requests:
          cpu: "4"
          memory: "4Gi"
          nvidia.com/gpu: "1"
    # Optional: Configure autoscaling behavior
    scaleTarget: 1
    minReplicas: 1
    maxReplicas: 3
