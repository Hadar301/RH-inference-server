# vllm-serving-runtime.yaml
apiVersion: serving.kserve.io/v1alpha1
kind: ServingRuntime
metadata:
  name: vllm-cuda-runtime
  namespace: hacohen-inference-server # Your OpenShift project/namespace
spec:
  supportedModelFormats:
    - name: vllm
      version: "1"
  protocolVersions:
    - v1
  builtInAdapter:
    server: http
    runtimeVersion: "1"
    ports:
      http: 8000
  containers:
    - name: kserve-container
      image: registry.redhat.io/rhaiis/vllm-cuda-rhel9:3.0.0
      resources:
        limits:
          cpu: "8"
          memory: "8Gi"
          nvidia.com/gpu: "1"
        requests:
          cpu: "4"
          memory: "4Gi"
          nvidia.com/gpu: "1"
      ports: 
        - containerPort: 8000
          name: http1
          protocol: TCP
      imagePullSecrets:
        - name: redhat-registry
      env:
        - name: HUGGING_FACE_HUB_TOKEN # Environment variable for vLLM
          valueFrom:
            secretKeyRef:
              name: hf-token-secret
              key: HUGGING_FACE_HUB_TOKEN # Matches the key in your secret
        - name: MODEL_ID # Tells vLLM which model to load from Hugging Face
          value: "RedHatAI/Llama-3.2-1B-Instruct-FP8"
      command: ["python", "-m", "vllm.entrypoints.openai.api_server"]
      args:
        - "--host"
        - "0.0.0.0"
        - "--port"
        - "8000"
        - "--model"
        - "RedHatAI/Llama-3.2-1B-Instruct-FP8"
        - "--served-model-name"
        - "RedHatAI/Llama-3.2-1B-Instruct-FP8"
      # --- START: Updated Probes to use HTTPGet ---
      startupProbe:
        httpGet:
          path: /health # Use the vLLM /health endpoint
          port: 8000
        initialDelaySeconds: 5
        periodSeconds: 10
        failureThreshold: 60 # Allow up to 10 minutes (60 * 10s) for startup
      readinessProbe:
        httpGet:
          path: /health # Use the vLLM /health endpoint
          port: 8000
        initialDelaySeconds: 180 # Give it more time after startupProbe for initial model loading
        periodSeconds: 10
        failureThreshold: 3
      # --- END: Updated Probes to use HTTPGet ---
  tolerations:
      - key: "p4-gpu" # dev02 is "g5-gpu" dev04 is "p4-gpu"
        operator: "Exists"
        effect: "NoSchedule"
      - key: "node-role.kubernetes.io/master"
        operator: "Exists"
        effect: "NoSchedule"